{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Iqra-01/Google_AI_studio_Genai/blob/main/templates/prompting_gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKwyTRdwB8aW"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RXInneX6xx7c"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.8.2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "kWIuwKG2_oWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daaa5309-51d3-4aa8-844e-555ff63c2c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"how does gemini work\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"model\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"Gemini is Google's most capable multimodal model. It's designed to understand and generate text, code, images, audio, and video \\u2013 all within a single model.  Here's a breakdown of how it works:\\\\n\\\\n**Key Concepts:**\\\\n\\\\n* **Multimodal:**  Gemini's core strength is its ability to process and integrate information from different modalities like text, images, audio, and video.  This allows it to perform tasks that require understanding the relationships between these modalities, such as describing an image, generating captions for videos, or answering questions about a combination of text and images.\\\\n\\\\n* **Large Language Model (LLM):** At its heart, Gemini is a large language model. LLMs are trained on massive datasets of text and code, enabling them to learn patterns, grammar, and context. This allows them to generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\\\\n\\\\n* **Transformer Architecture:** Gemini likely utilizes a transformer architecture, which is a neural network architecture designed to handle sequential data like text and audio. Transformers are particularly good at capturing long-range dependencies in data, which is crucial for understanding complex sentences and generating coherent responses.\\\\n\\\\n* **Multimodal Training:**  Gemini's multimodal capabilities come from its training on diverse datasets containing various modalities. This training likely involves learning joint representations of different modalities, allowing the model to understand the relationships between them.  For instance, the model might learn the association between the image of a cat and the word \\\\\\\"cat\\\\\\\".\\\\n\\\\n* **Fine-tuning:**  Like other large models, Gemini can be fine-tuned for specific tasks. This involves training the model further on a smaller, task-specific dataset.  Fine-tuning allows the model to adapt to particular domains and improve its performance on those tasks.\\\\n\\\\n\\\\n**Simplified Workflow (Illustrative Example):**\\\\n\\\\nImagine you ask Gemini, \\\\\\\"What breed of dog is this?\\\\\\\" and provide a picture of a Golden Retriever.\\\\n\\\\n1. **Input Processing:** Gemini processes the input image and your text question.  The image is converted into a numerical representation that the model can understand.\\\\n\\\\n2. **Multimodal Understanding:** Gemini combines the information from the image and the text question. Its multimodal capabilities allow it to understand that the question refers to the dog in the image.\\\\n\\\\n3. **Information Retrieval and Reasoning:** Gemini uses its knowledge gained during training to identify the breed of dog in the image.  This involves accessing and processing information about dog breeds and matching it with the visual features extracted from the image.\\\\n\\\\n4. **Output Generation:** Gemini generates a text response, such as \\\\\\\"That's a Golden Retriever.\\\\\\\"\\\\n\\\\n\\\\n**Key Differences from Other Models:**\\\\n\\\\nGemini stands out because of its native multimodal capabilities. Unlike some models that handle different modalities separately, Gemini is designed to process and integrate them seamlessly. This allows for more sophisticated applications and a deeper understanding of the relationships between different forms of information.\\\\n\\\\n**Limitations:**\\\\n\\\\nLike all AI models, Gemini has limitations. It can sometimes generate incorrect or biased information, hallucinate details, or struggle with nuanced reasoning.  Ongoing research and development are aimed at addressing these limitations.\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"tell me more about llms\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"model\",\n",
            "        \"parts\": [\n",
            "            {\n",
            "                \"text\": \"Large Language Models (LLMs) are a type of artificial intelligence (AI) that are designed to understand and generate human language. They are trained on massive amounts of text data, allowing them to learn patterns, grammar, semantics, and even some world knowledge.\\\\n\\\\nHere's a breakdown of key aspects of LLMs:\\\\n\\\\n**1. Training Data and Process:**\\\\n\\\\n* **Massive Datasets:** LLMs are trained on incredibly large datasets of text and code, often petabytes in size. This data can come from various sources like books, articles, websites, code repositories, and more.\\\\n* **Self-Supervised Learning:**  A common training method is self-supervised learning, where the model learns by predicting masked words or phrases within a sentence. This allows the model to learn the relationships between words and develop a rich understanding of language.\\\\n* **Transformer Architecture:** Most modern LLMs are based on the transformer architecture, a neural network architecture particularly effective at processing sequential data like text. Transformers utilize a mechanism called \\\\\\\"attention\\\\\\\" which allows the model to weigh the importance of different words in a sentence when generating a response.\\\\n\\\\n**2. Capabilities:**\\\\n\\\\n* **Text Generation:** LLMs can generate various types of text, including stories, poems, articles, summaries, and code.\\\\n* **Translation:**  They can translate between multiple languages.\\\\n* **Question Answering:** They can answer questions based on the information they have learned from their training data.\\\\n* **Dialogue:** They can engage in conversations and generate contextually relevant responses.\\\\n* **Code Generation:**  LLMs can generate code in various programming languages and even explain existing code.\\\\n* **Summarization:**  They can condense longer texts into concise summaries.\\\\n\\\\n**3. Key Concepts:**\\\\n\\\\n* **Context Window:** LLMs have a limited \\\\\\\"context window,\\\\\\\" meaning they can only consider a certain amount of text at a time.  Anything beyond this window is not directly considered when generating a response.\\\\n* **Tokenization:** Input text is broken down into smaller units called \\\\\\\"tokens,\\\\\\\" which can be words, subwords, or even characters. This allows the model to process and represent the text numerically.\\\\n* **Embeddings:** Words and phrases are represented as numerical vectors called \\\\\\\"embeddings,\\\\\\\" capturing their semantic meaning and relationships with other words.\\\\n* **Prompt Engineering:**  The process of crafting effective input prompts to guide the LLM towards desired outputs.  A well-crafted prompt can significantly improve the quality and relevance of the generated text.\\\\n\\\\n**4. Examples of LLMs:**\\\\n\\\\n* **GPT (Generative Pre-trained Transformer) models by OpenAI:**  A popular series of LLMs known for their powerful text generation capabilities.\\\\n* **LaMDA (Language Model for Dialogue Applications) by Google:**  Focused on conversational AI and generating natural and engaging dialogue.\\\\n* **PaLM (Pathways Language Model) by Google:** A large language model capable of performing a wide range of tasks, including language understanding, reasoning, and code generation.\\\\n* **LLaMA (Large Language Model Meta AI) by Meta:**  An open-source LLM designed to be more accessible to researchers.\\\\n\\\\n**5. Limitations:**\\\\n\\\\n* **Bias:** LLMs can inherit biases present in their training data, potentially leading to unfair or discriminatory outputs.\\\\n* **Hallucinations:** They can sometimes generate factually incorrect or nonsensical information, often referred to as \\\\\\\"hallucinations.\\\\\\\"\\\\n* **Lack of Real-World Understanding:**  While LLMs can learn a lot from text, they lack true understanding of the real world and can struggle with common sense reasoning.\\\\n* **Ethical Concerns:**  The potential misuse of LLMs for generating misinformation, spam, or harmful content raises ethical concerns.\\\\n\\\\n\\\\nLLMs are constantly evolving, and ongoing research aims to address their limitations and improve their capabilities. They represent a significant advancement in AI and have the potential to transform various fields, from education and healthcare to entertainment and software development.\\\\n\"\n",
            "            }\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# import necessary modules.\n",
        "import base64\n",
        "import copy\n",
        "import json\n",
        "import pathlib\n",
        "import requests\n",
        "\n",
        "\n",
        "import PIL.Image\n",
        "import IPython.display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "try:\n",
        "    # The SDK will automatically read it from the GOOGLE_API_KEY environment variable.\n",
        "    # In Colab get the key from Colab-secrets (\"üîë\" in the left panel).\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Parse the arguments\n",
        "\n",
        "model = 'gemini-1.5-pro' # @param {isTemplate: true}\n",
        "contents_b64 = '[{"role":"user","parts":[{"text":"how does gemini work\\n"}]},{"role":"model","parts":[{"text":"Gemini is Google's most capable multimodal model. It's designed to understand and generate text, code, images, audio, and video – all within a single model.  Here's a breakdown of how it works:\\n\\n**Key Concepts:**\\n\\n* **Multimodal:**  Gemini's core strength is its ability to process and integrate information from different modalities like text, images, audio, and video.  This allows it to perform tasks that require understanding the relationships between these modalities, such as describing an image, generating captions for videos, or answering questions about a combination of text and images.\\n\\n* **Large Language Model (LLM):** At its heart, Gemini is a large language model. LLMs are trained on massive datasets of text and code, enabling them to learn patterns, grammar, and context. This allows them to generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\\n\\n* **Transformer Architecture:** Gemini likely utilizes a transformer architecture, which is a neural network architecture designed to handle sequential data like text and audio. Transformers are particularly good at capturing long-range dependencies in data, which is crucial for understanding complex sentences and generating coherent responses.\\n\\n* **Multimodal Training:**  Gemini's multimodal capabilities come from its training on diverse datasets containing various modalities. This training likely involves learning joint representations of different modalities, allowing the model to understand the relationships between them.  For instance, the model might learn the association between the image of a cat and the word \\\"cat\\\".\\n\\n* **Fine-tuning:**  Like other large models, Gemini can be fine-tuned for specific tasks. This involves training the model further on a smaller, task-specific dataset.  Fine-tuning allows the model to adapt to particular domains and improve its performance on those tasks.\\n\\n\\n**Simplified Workflow (Illustrative Example):**\\n\\nImagine you ask Gemini, \\\"What breed of dog is this?\\\" and provide a picture of a Golden Retriever.\\n\\n1. **Input Processing:** Gemini processes the input image and your text question.  The image is converted into a numerical representation that the model can understand.\\n\\n2. **Multimodal Understanding:** Gemini combines the information from the image and the text question. Its multimodal capabilities allow it to understand that the question refers to the dog in the image.\\n\\n3. **Information Retrieval and Reasoning:** Gemini uses its knowledge gained during training to identify the breed of dog in the image.  This involves accessing and processing information about dog breeds and matching it with the visual features extracted from the image.\\n\\n4. **Output Generation:** Gemini generates a text response, such as \\\"That's a Golden Retriever.\\\"\\n\\n\\n**Key Differences from Other Models:**\\n\\nGemini stands out because of its native multimodal capabilities. Unlike some models that handle different modalities separately, Gemini is designed to process and integrate them seamlessly. This allows for more sophisticated applications and a deeper understanding of the relationships between different forms of information.\\n\\n**Limitations:**\\n\\nLike all AI models, Gemini has limitations. It can sometimes generate incorrect or biased information, hallucinate details, or struggle with nuanced reasoning.  Ongoing research and development are aimed at addressing these limitations.\\n"}]},{"role":"user","parts":[{"text":"tell me more about llms\\n"}]},{"role":"model","parts":[{"text":"Large Language Models (LLMs) are a type of artificial intelligence (AI) that are designed to understand and generate human language. They are trained on massive amounts of text data, allowing them to learn patterns, grammar, semantics, and even some world knowledge.\\n\\nHere's a breakdown of key aspects of LLMs:\\n\\n**1. Training Data and Process:**\\n\\n* **Massive Datasets:** LLMs are trained on incredibly large datasets of text and code, often petabytes in size. This data can come from various sources like books, articles, websites, code repositories, and more.\\n* **Self-Supervised Learning:**  A common training method is self-supervised learning, where the model learns by predicting masked words or phrases within a sentence. This allows the model to learn the relationships between words and develop a rich understanding of language.\\n* **Transformer Architecture:** Most modern LLMs are based on the transformer architecture, a neural network architecture particularly effective at processing sequential data like text. Transformers utilize a mechanism called \\\"attention\\\" which allows the model to weigh the importance of different words in a sentence when generating a response.\\n\\n**2. Capabilities:**\\n\\n* **Text Generation:** LLMs can generate various types of text, including stories, poems, articles, summaries, and code.\\n* **Translation:**  They can translate between multiple languages.\\n* **Question Answering:** They can answer questions based on the information they have learned from their training data.\\n* **Dialogue:** They can engage in conversations and generate contextually relevant responses.\\n* **Code Generation:**  LLMs can generate code in various programming languages and even explain existing code.\\n* **Summarization:**  They can condense longer texts into concise summaries.\\n\\n**3. Key Concepts:**\\n\\n* **Context Window:** LLMs have a limited \\\"context window,\\\" meaning they can only consider a certain amount of text at a time.  Anything beyond this window is not directly considered when generating a response.\\n* **Tokenization:** Input text is broken down into smaller units called \\\"tokens,\\\" which can be words, subwords, or even characters. This allows the model to process and represent the text numerically.\\n* **Embeddings:** Words and phrases are represented as numerical vectors called \\\"embeddings,\\\" capturing their semantic meaning and relationships with other words.\\n* **Prompt Engineering:**  The process of crafting effective input prompts to guide the LLM towards desired outputs.  A well-crafted prompt can significantly improve the quality and relevance of the generated text.\\n\\n**4. Examples of LLMs:**\\n\\n* **GPT (Generative Pre-trained Transformer) models by OpenAI:**  A popular series of LLMs known for their powerful text generation capabilities.\\n* **LaMDA (Language Model for Dialogue Applications) by Google:**  Focused on conversational AI and generating natural and engaging dialogue.\\n* **PaLM (Pathways Language Model) by Google:** A large language model capable of performing a wide range of tasks, including language understanding, reasoning, and code generation.\\n* **LLaMA (Large Language Model Meta AI) by Meta:**  An open-source LLM designed to be more accessible to researchers.\\n\\n**5. Limitations:**\\n\\n* **Bias:** LLMs can inherit biases present in their training data, potentially leading to unfair or discriminatory outputs.\\n* **Hallucinations:** They can sometimes generate factually incorrect or nonsensical information, often referred to as \\\"hallucinations.\\\"\\n* **Lack of Real-World Understanding:**  While LLMs can learn a lot from text, they lack true understanding of the real world and can struggle with common sense reasoning.\\n* **Ethical Concerns:**  The potential misuse of LLMs for generating misinformation, spam, or harmful content raises ethical concerns.\\n\\n\\nLLMs are constantly evolving, and ongoing research aims to address their limitations and improve their capabilities. They represent a significant advancement in AI and have the potential to transform various fields, from education and healthcare to entertainment and software development.\\n"}]}]' # @param {isTemplate: true}\n",
        "generation_config_b64 = 'eyJ0ZW1wZXJhdHVyZSI6MSwidG9wX3AiOjAuOTUsInRvcF9rIjo0MCwibWF4X291dHB1dF90b2tlbnMiOjgxOTJ9' # @param {isTemplate: true}\n",
        "safety_settings_b64 = \"e30=\"  # @param {isTemplate: true}\n",
        "\n",
        "gais_contents = json.loads(base64.b64decode(contents_b64))\n",
        "\n",
        "generation_config = json.loads(base64.b64decode(generation_config_b64))\n",
        "safety_settings = json.loads(base64.b64decode(safety_settings_b64))\n",
        "\n",
        "stream = False\n",
        "\n",
        "# Convert and upload the files\n",
        "\n",
        "tempfiles = pathlib.Path(f\"tempfiles\")\n",
        "tempfiles.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "drive = None\n",
        "def upload_file_data(file_data, index):\n",
        "    \"\"\"Upload files to the Files API.\n",
        "\n",
        "    For each file, Google AI Studio either sent:\n",
        "    - a Google Drive ID,\n",
        "    - a URL,\n",
        "    - a file path, or\n",
        "    - The raw bytes (`inline_data`).\n",
        "\n",
        "    The API only understands `inline_data` or it's Files API.\n",
        "    This code, uploads files to the files API where the API can access them.\n",
        "    \"\"\"\n",
        "\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "    if drive_id := file_data.pop(\"drive_id\", None):\n",
        "        if drive is None:\n",
        "          from google.colab import drive\n",
        "          drive.mount(\"/gdrive\")\n",
        "\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        print(\"Uploading:\", str(path))\n",
        "        file_info = genai.upload_file(path=path, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if url := file_data.pop(\"url\", None):\n",
        "        response = requests.get(url)\n",
        "        data = response.content\n",
        "        name = url.split(\"/\")[-1]\n",
        "        path = tempfiles / str(index)\n",
        "        path.write_bytes(data)\n",
        "        print(\"Uploading:\", url)\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files \"\n",
        "                'to Colab using the file manager (\"üìÅ Files\" in the left '\n",
        "                \"toolbar)\"\n",
        "            )\n",
        "        file_info = genai.upload_file(path, display_name=name, mime_type=mime_type)\n",
        "        file_data[\"file_uri\"] = file_info.uri\n",
        "        return\n",
        "\n",
        "    if \"inline_data\" in file_data:\n",
        "        return\n",
        "\n",
        "    raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "\n",
        "contents = copy.deepcopy(gais_contents)\n",
        "\n",
        "index = 0\n",
        "for content in contents:\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if file_data := part.get(\"file_data\", None):\n",
        "            upload_file_data(file_data, index)\n",
        "            index += 1\n",
        "\n",
        "import json\n",
        "print(json.dumps(contents, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7zAD69vE92b"
      },
      "source": [
        "## Call `generate_content`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LB2LxPmAB95V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acc0e97-e13b-449b-d90b-ea266935960b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: No candidates returned. The prompt may have been blocked.\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# Call the model and print the response.\n",
        "gemini = genai.GenerativeModel(model_name=model)\n",
        "\n",
        "response = gemini.generate_content(\n",
        "    contents,\n",
        "    generation_config=generation_config,\n",
        "    safety_settings=safety_settings,\n",
        "    stream=stream,\n",
        ")\n",
        "\n",
        "if response.candidates:\n",
        "    display(Markdown(response.text))\n",
        "else:\n",
        "    # Print prompt feedback if available to understand the blocking reason\n",
        "    if response.prompt_feedback:\n",
        "        print(f\"Prompt feedback: {response.prompt_feedback}\")\n",
        "    else:\n",
        "        print(\"Error: No candidates returned. The prompt may have been blocked.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c9d345e9868"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />Docs on ai.google.dev</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google-gemini/cookbook/blob/main/quickstarts\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />More notebooks in the Cookbook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F91AeeGO1ncU"
      },
      "source": [
        "## [optional] Show the conversation\n",
        "\n",
        "This section displays the conversation received from Google AI Studio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yoL3p3KPylFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db168278-87e2-4cc1-cab4-2fdf6c8d39cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Role: user \n",
            "\n",
            "how does gemini work\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n",
            "Role: model \n",
            "\n",
            "Gemini is Google's most capable multimodal model. It's designed to understand and generate text, code, images, audio, and video ‚Äì all within a single model.  Here's a breakdown of how it works:\\n\\n**Key Concepts:**\\n\\n* **Multimodal:**  Gemini's core strength is its ability to process and integrate information from different modalities like text, images, audio, and video.  This allows it to perform tasks that require understanding the relationships between these modalities, such as describing an image, generating captions for videos, or answering questions about a combination of text and images.\\n\\n* **Large Language Model (LLM):** At its heart, Gemini is a large language model. LLMs are trained on massive datasets of text and code, enabling them to learn patterns, grammar, and context. This allows them to generate human-quality text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\\n\\n* **Transformer Architecture:** Gemini likely utilizes a transformer architecture, which is a neural network architecture designed to handle sequential data like text and audio. Transformers are particularly good at capturing long-range dependencies in data, which is crucial for understanding complex sentences and generating coherent responses.\\n\\n* **Multimodal Training:**  Gemini's multimodal capabilities come from its training on diverse datasets containing various modalities. This training likely involves learning joint representations of different modalities, allowing the model to understand the relationships between them.  For instance, the model might learn the association between the image of a cat and the word \\\"cat\\\".\\n\\n* **Fine-tuning:**  Like other large models, Gemini can be fine-tuned for specific tasks. This involves training the model further on a smaller, task-specific dataset.  Fine-tuning allows the model to adapt to particular domains and improve its performance on those tasks.\\n\\n\\n**Simplified Workflow (Illustrative Example):**\\n\\nImagine you ask Gemini, \\\"What breed of dog is this?\\\" and provide a picture of a Golden Retriever.\\n\\n1. **Input Processing:** Gemini processes the input image and your text question.  The image is converted into a numerical representation that the model can understand.\\n\\n2. **Multimodal Understanding:** Gemini combines the information from the image and the text question. Its multimodal capabilities allow it to understand that the question refers to the dog in the image.\\n\\n3. **Information Retrieval and Reasoning:** Gemini uses its knowledge gained during training to identify the breed of dog in the image.  This involves accessing and processing information about dog breeds and matching it with the visual features extracted from the image.\\n\\n4. **Output Generation:** Gemini generates a text response, such as \\\"That's a Golden Retriever.\\\"\\n\\n\\n**Key Differences from Other Models:**\\n\\nGemini stands out because of its native multimodal capabilities. Unlike some models that handle different modalities separately, Gemini is designed to process and integrate them seamlessly. This allows for more sophisticated applications and a deeper understanding of the relationships between different forms of information.\\n\\n**Limitations:**\\n\\nLike all AI models, Gemini has limitations. It can sometimes generate incorrect or biased information, hallucinate details, or struggle with nuanced reasoning.  Ongoing research and development are aimed at addressing these limitations.\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n",
            "Role: user \n",
            "\n",
            "tell me more about llms\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n",
            "Role: model \n",
            "\n",
            "Large Language Models (LLMs) are a type of artificial intelligence (AI) that are designed to understand and generate human language. They are trained on massive amounts of text data, allowing them to learn patterns, grammar, semantics, and even some world knowledge.\\n\\nHere's a breakdown of key aspects of LLMs:\\n\\n**1. Training Data and Process:**\\n\\n* **Massive Datasets:** LLMs are trained on incredibly large datasets of text and code, often petabytes in size. This data can come from various sources like books, articles, websites, code repositories, and more.\\n* **Self-Supervised Learning:**  A common training method is self-supervised learning, where the model learns by predicting masked words or phrases within a sentence. This allows the model to learn the relationships between words and develop a rich understanding of language.\\n* **Transformer Architecture:** Most modern LLMs are based on the transformer architecture, a neural network architecture particularly effective at processing sequential data like text. Transformers utilize a mechanism called \\\"attention\\\" which allows the model to weigh the importance of different words in a sentence when generating a response.\\n\\n**2. Capabilities:**\\n\\n* **Text Generation:** LLMs can generate various types of text, including stories, poems, articles, summaries, and code.\\n* **Translation:**  They can translate between multiple languages.\\n* **Question Answering:** They can answer questions based on the information they have learned from their training data.\\n* **Dialogue:** They can engage in conversations and generate contextually relevant responses.\\n* **Code Generation:**  LLMs can generate code in various programming languages and even explain existing code.\\n* **Summarization:**  They can condense longer texts into concise summaries.\\n\\n**3. Key Concepts:**\\n\\n* **Context Window:** LLMs have a limited \\\"context window,\\\" meaning they can only consider a certain amount of text at a time.  Anything beyond this window is not directly considered when generating a response.\\n* **Tokenization:** Input text is broken down into smaller units called \\\"tokens,\\\" which can be words, subwords, or even characters. This allows the model to process and represent the text numerically.\\n* **Embeddings:** Words and phrases are represented as numerical vectors called \\\"embeddings,\\\" capturing their semantic meaning and relationships with other words.\\n* **Prompt Engineering:**  The process of crafting effective input prompts to guide the LLM towards desired outputs.  A well-crafted prompt can significantly improve the quality and relevance of the generated text.\\n\\n**4. Examples of LLMs:**\\n\\n* **GPT (Generative Pre-trained Transformer) models by OpenAI:**  A popular series of LLMs known for their powerful text generation capabilities.\\n* **LaMDA (Language Model for Dialogue Applications) by Google:**  Focused on conversational AI and generating natural and engaging dialogue.\\n* **PaLM (Pathways Language Model) by Google:** A large language model capable of performing a wide range of tasks, including language understanding, reasoning, and code generation.\\n* **LLaMA (Large Language Model Meta AI) by Meta:**  An open-source LLM designed to be more accessible to researchers.\\n\\n**5. Limitations:**\\n\\n* **Bias:** LLMs can inherit biases present in their training data, potentially leading to unfair or discriminatory outputs.\\n* **Hallucinations:** They can sometimes generate factually incorrect or nonsensical information, often referred to as \\\"hallucinations.\\\"\\n* **Lack of Real-World Understanding:**  While LLMs can learn a lot from text, they lack true understanding of the real world and can struggle with common sense reasoning.\\n* **Ethical Concerns:**  The potential misuse of LLMs for generating misinformation, spam, or harmful content raises ethical concerns.\\n\\n\\nLLMs are constantly evolving, and ongoing research aims to address their limitations and improve their capabilities. They represent a significant advancement in AI and have the potential to transform various fields, from education and healthcare to entertainment and software development.\\n \n",
            "\n",
            "-------------------------------------------------------------------------------- \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Show the conversation, in colab.\n",
        "import mimetypes\n",
        "\n",
        "def show_file(file_data):\n",
        "    mime_type = file_data[\"mime_type\"]\n",
        "\n",
        "    if drive_id := file_data.get(\"drive_id\", None):\n",
        "        path = next(\n",
        "            pathlib.Path(f\"/gdrive/.shortcut-targets-by-id/{drive_id}\").glob(\"*\")\n",
        "        )\n",
        "        name = path\n",
        "        # data = path.read_bytes()\n",
        "        kwargs = {\"filename\": path}\n",
        "    elif url := file_data.get(\"url\", None):\n",
        "        name = url\n",
        "        kwargs = {\"url\": url}\n",
        "        # response = requests.get(url)\n",
        "        # data = response.content\n",
        "    elif data := file_data.get(\"inline_data\", None):\n",
        "        name = None\n",
        "        kwargs = {\"data\": data}\n",
        "    elif name := file_data.get(\"filename\", None):\n",
        "        if not pathlib.Path(name).exists():\n",
        "            raise IOError(\n",
        "                f\"local file: `{name}` does not exist. You can upload files to \"\n",
        "                'Colab using the file manager (\"üìÅ Files\"in the left toolbar)'\n",
        "            )\n",
        "    else:\n",
        "        raise ValueError(\"Either `drive_id`, `url` or `inline_data` must be provided.\")\n",
        "\n",
        "        print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "        return\n",
        "\n",
        "    format = mimetypes.guess_extension(mime_type).strip(\".\")\n",
        "    if mime_type.startswith(\"image/\"):\n",
        "        image = IPython.display.Image(**kwargs, width=256)\n",
        "        IPython.display.display(image)\n",
        "        print()\n",
        "        return\n",
        "\n",
        "    if mime_type.startswith(\"audio/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Audio(**kwargs)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    if mime_type.startswith(\"video/\"):\n",
        "        if len(data) < 2**12:\n",
        "            audio = IPython.display.Video(**kwargs, mimetype=mime_type)\n",
        "            IPython.display.display(audio)\n",
        "            print()\n",
        "            return\n",
        "\n",
        "    print(f\"File:\\n    name: {name}\\n    mime_type: {mime_type}\\n\")\n",
        "\n",
        "\n",
        "for content in gais_contents:\n",
        "    if role := content.get(\"role\", None):\n",
        "        print(\"Role:\", role, \"\\n\")\n",
        "\n",
        "    for n, part in enumerate(content[\"parts\"]):\n",
        "        if text := part.get(\"text\", None):\n",
        "            print(text, \"\\n\")\n",
        "\n",
        "        elif file_data := part.get(\"file_data\", None):\n",
        "            show_file(file_data)\n",
        "\n",
        "    print(\"-\" * 80, \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "aistudio_gemini_prompt_freeform.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}